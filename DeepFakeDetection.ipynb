{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project : Develop model to detect deepfake video with Highest accuracy (possible by us) which has explainability . Will try to create few model to generate comparisons for comparison and then picking one final one as our \"final model\".\n",
    "\n",
    "Business Value: Flagging misinformation/ protecting digital identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3822538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in ./deepfake_env/lib/python3.14/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./deepfake_env/lib/python3.14/site-packages (from optuna) (1.17.2)\n",
      "Requirement already satisfied: colorlog in ./deepfake_env/lib/python3.14/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in ./deepfake_env/lib/python3.14/site-packages (from optuna) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./deepfake_env/lib/python3.14/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in ./deepfake_env/lib/python3.14/site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in ./deepfake_env/lib/python3.14/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in ./deepfake_env/lib/python3.14/site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in ./deepfake_env/lib/python3.14/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in ./deepfake_env/lib/python3.14/site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./deepfake_env/lib/python3.14/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchcodec in ./deepfake_env/lib/python3.14/site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in ./deepfake_env/lib/python3.14/site-packages (from gdown) (4.14.3)\n",
      "Requirement already satisfied: filelock in ./deepfake_env/lib/python3.14/site-packages (from gdown) (3.20.0)\n",
      "Requirement already satisfied: requests[socks] in ./deepfake_env/lib/python3.14/site-packages (from gdown) (2.32.5)\n",
      "Requirement already satisfied: tqdm in ./deepfake_env/lib/python3.14/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./deepfake_env/lib/python3.14/site-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./deepfake_env/lib/python3.14/site-packages (from beautifulsoup4->gdown) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./deepfake_env/lib/python3.14/site-packages (from requests[socks]->gdown) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./deepfake_env/lib/python3.14/site-packages (from requests[socks]->gdown) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./deepfake_env/lib/python3.14/site-packages (from requests[socks]->gdown) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./deepfake_env/lib/python3.14/site-packages (from requests[socks]->gdown) (2025.11.12)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, gdown\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [gdown]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PySocks-1.7.1 gdown-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8.\n          2. The PyTorch version (2.9.1) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core8.dylib\nFFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core7.dylib\nFFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core6.dylib\nFFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core5.dylib\nFFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core4.dylib\n[end of libtorchcodec loading traceback].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms, models\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtimm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Computer Vision\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/__init__.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Note: usort wants to put Frame and FrameBatch after decoders and samplers,\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# but that results in circular import.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_frame\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioSamples, Frame, FrameBatch  \u001b[38;5;66;03m# usort:skip # noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decoders, encoders, samplers  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Note that version.py is generated during install.\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/decoders/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioStreamMetadata, VideoStreamMetadata\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_audio_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decoder_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_cuda_backend  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/_core/__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     AudioStreamMetadata,\n\u001b[32m     10\u001b[39m     ContainerMetadata,\n\u001b[32m     11\u001b[39m     get_container_metadata,\n\u001b[32m     12\u001b[39m     get_container_metadata_from_header,\n\u001b[32m     13\u001b[39m     VideoStreamMetadata,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     _add_video_stream,\n\u001b[32m     17\u001b[39m     _get_backend_details,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     seek_to_pts,\n\u001b[32m     46\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/_core/_metadata.py:16\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _get_container_json_metadata,\n\u001b[32m     18\u001b[39m     _get_stream_json_metadata,\n\u001b[32m     19\u001b[39m     create_from_file,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     23\u001b[39m SPACES = \u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mStreamMetadata\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/_core/ops.py:85\u001b[39m\n\u001b[32m     65\u001b[39m     traceback = (\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[32m     68\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[end of libtorchcodec loading traceback].\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m     )\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     71\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33m          1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m ffmpeg_major_version, core_library_path = \u001b[43mload_torchcodec_shared_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Note: We use disallow_in_graph because PyTorch does constant propagation of\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# factory functions.\u001b[39;00m\n\u001b[32m     90\u001b[39m create_from_file = torch._dynamo.disallow_in_graph(\n\u001b[32m     91\u001b[39m     torch.ops.torchcodec_ns.create_from_file.default\n\u001b[32m     92\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/_core/ops.py:70\u001b[39m, in \u001b[36mload_torchcodec_shared_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     63\u001b[39m         exceptions.append((ffmpeg_major_version, e))\n\u001b[32m     65\u001b[39m traceback = (\n\u001b[32m     66\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[start of libtorchcodec loading traceback]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFFmpeg version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, e \u001b[38;5;129;01min\u001b[39;00m exceptions)\n\u001b[32m     68\u001b[39m     + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[end of libtorchcodec loading traceback].\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     71\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mCould not load libtorchcodec. Likely causes:\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33m      1. FFmpeg is not properly installed in your environment. We support\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[33m         versions 4, 5, 6, 7, and 8.\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[33m      2. The PyTorch version (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) is not compatible with\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[33m         this version of TorchCodec. Refer to the version compatibility\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[33m         table:\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[33m         https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[33m      3. Another runtime dependency; see exceptions below.\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m    The following exceptions were raised as we tried to load libtorchcodec:\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8.\n          2. The PyTorch version (2.9.1) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core8.dylib\nFFmpeg version 7: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core7.dylib\nFFmpeg version 6: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core6.dylib\nFFmpeg version 5: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core5.dylib\nFFmpeg version 4: Could not load this library: /Users/mohini.gangaram/Desktop/MLPostGrad/Sem3/Deep Learning/Final Project/deepfake_env/lib/python3.14/site-packages/torchcodec/libtorchcodec_core4.dylib\n[end of libtorchcodec loading traceback]."
     ]
    }
   ],
   "source": [
    "# # Deep Fake Detection Project\n",
    "# ## Complete Pipeline: Data Analysis ‚Üí Feature Engineering ‚Üí Model Training ‚Üí Hyperparameter Tuning\n",
    "#\n",
    "# **Dataset**: Google Drive - Celeb-Real, Celeb-Fake, and Testing folders\n",
    "# - **Celeb-Real**: Real/Original videos (Label: 0, \"Celeb-Real\")\n",
    "# - **Celeb-Fake**: Fake/Manipulated videos (Label: 1, \"Fake\")\n",
    "# - **Testing**: Test videos for evaluation\n",
    "#\n",
    "# **Dataset Link**: https://drive.google.com/drive/folders/1nBKjUpi2wQyMfWDuNsreqY11DVZrbk7x\n",
    "#\n",
    "# **Objective**: Detect original vs AI-generated images and videos\n",
    "#\n",
    "# **Approach**:\n",
    "# - Comprehensive EDA\n",
    "# - Feature engineering (spatial, frequency, texture features)\n",
    "# - Multiple CNN architectures + Transfer Learning\n",
    "# - Hyperparameter optimization\n",
    "# - Model evaluation and comparison\n",
    "#\n",
    "%pip install optuna\n",
    "%pip install gdown\n",
    "# Note: torchcodec is not needed - we use OpenCV for video processing\n",
    "# %%\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import timm\n",
    "# torchcodec not needed - using OpenCV for video processing instead\n",
    "# import torchcodec  # Optional: requires FFmpeg installation\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import feature, filters\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# ML & Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# HuggingFace\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Utilities\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Using CPU - training will be slower\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b95544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. Data Loading from Google Drive Folders\n",
    "# Dataset Structure:\n",
    "# - celeb-real/: Real videos (label: 0, \"Celeb-Real\")\n",
    "# - celeb-fake/: Fake videos (label: 1, \"Fake\")\n",
    "# - testing/: Test videos (for evaluation)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATASET FROM GOOGLE DRIVE FOLDERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Install gdown for downloading from Google Drive\n",
    "%pip install gdown\n",
    "\n",
    "import gdown\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# Set up data directory\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Google Drive folder IDs (extracted from the share link)\n",
    "GOOGLE_DRIVE_FOLDER_ID = \"1nBKjUpi2wQyMfWDuNsreqY11DVZrbk7x\"\n",
    "CELEB_REAL_FOLDER = DATA_DIR / \"Celeb-Real\"\n",
    "CELEB_FAKE_FOLDER = DATA_DIR / \"Celeb-Fake\"\n",
    "TESTING_FOLDER = DATA_DIR / \"Testing\"\n",
    "\n",
    "def download_from_google_drive(folder_id, output_dir):\n",
    "    \"\"\"Download folder from Google Drive using gdown\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Downloading from Google Drive folder: {folder_id}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Download folder as zip\n",
    "    url = f\"https://drive.google.com/uc?id={folder_id}\"\n",
    "    zip_path = output_dir / \"dataset.zip\"\n",
    "    \n",
    "    try:\n",
    "        gdown.download_folder(url, output=str(output_dir), quiet=False, use_cookies=False)\n",
    "        print(f\"‚úÖ Downloaded to {output_dir}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Download error: {e}\")\n",
    "        print(\"Please download manually from:\")\n",
    "        print(f\"https://drive.google.com/drive/folders/{folder_id}\")\n",
    "        print(f\"Extract to: {output_dir}\")\n",
    "        return False\n",
    "\n",
    "def load_videos_from_folder(folder_path, label, label_name):\n",
    "    \"\"\"Load all video files from a folder\"\"\"\n",
    "    folder_path = Path(folder_path)\n",
    "    if not folder_path.exists():\n",
    "        return []\n",
    "    \n",
    "    # Supported video formats\n",
    "    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.flv', '*.wmv', '*.webm']\n",
    "    video_files = []\n",
    "    \n",
    "    for ext in video_extensions:\n",
    "        video_files.extend(glob.glob(str(folder_path / \"**\" / ext), recursive=True))\n",
    "    \n",
    "    # Create data entries\n",
    "    data_list = []\n",
    "    for video_path in video_files:\n",
    "        data_list.append({\n",
    "            'video_path': video_path,\n",
    "            'label': label,\n",
    "            'label_name': label_name,\n",
    "            'folder': folder_path.name\n",
    "        })\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "# Download or check for local dataset\n",
    "print(\"\\n[INFO] Checking for dataset folders...\")\n",
    "print(f\"Celeb-Real folder: {CELEB_REAL_FOLDER}\")\n",
    "print(f\"Celeb-Fake folder: {CELEB_FAKE_FOLDER}\")\n",
    "print(f\"Testing folder: {TESTING_FOLDER}\")\n",
    "\n",
    "# Check if folders exist locally\n",
    "if not (CELEB_REAL_FOLDER.exists() and CELEB_FAKE_FOLDER.exists()):\n",
    "    print(\"\\n[INFO] Dataset folders not found locally.\")\n",
    "    print(\"Attempting to download from Google Drive...\")\n",
    "    print(\"Note: If download fails, please download manually and place folders in 'data/' directory\")\n",
    "    \n",
    "    # Try to download (this might not work for large folders, manual download recommended)\n",
    "    download_success = download_from_google_drive(GOOGLE_DRIVE_FOLDER_ID, DATA_DIR)\n",
    "    \n",
    "    if not download_success:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MANUAL DOWNLOAD REQUIRED\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Please follow these steps:\")\n",
    "        print(f\"1. Open: https://drive.google.com/drive/folders/{GOOGLE_DRIVE_FOLDER_ID}\")\n",
    "        print(\"2. Download the three folders: Celeb-Real, Celeb-Fake, Testing\")\n",
    "        print(f\"3. Extract them to: {DATA_DIR.absolute()}\")\n",
    "        print(\"4. Ensure folder structure:\")\n",
    "        print(f\"   {DATA_DIR}/Celeb-Real/\")\n",
    "        print(f\"   {DATA_DIR}/Celeb-Fake/\")\n",
    "        print(f\"   {DATA_DIR}/Testing/\")\n",
    "        print(\"\\nThen re-run this cell.\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset folders found locally!\")\n",
    "\n",
    "# Load videos from folders\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING VIDEOS FROM FOLDERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load training data (Celeb-Real and Celeb-Fake)\n",
    "train_data_list = []\n",
    "train_data_list.extend(load_videos_from_folder(CELEB_REAL_FOLDER, label=0, label_name=\"Celeb-Real\"))\n",
    "train_data_list.extend(load_videos_from_folder(CELEB_FAKE_FOLDER, label=1, label_name=\"Fake\"))\n",
    "\n",
    "# Load test data (Testing folder)\n",
    "test_data_list = load_videos_from_folder(TESTING_FOLDER, label=None, label_name=\"Testing\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "train_df = pd.DataFrame(train_data_list)\n",
    "test_df = pd.DataFrame(test_data_list)\n",
    "\n",
    "print(f\"\\n‚úÖ Training videos loaded: {len(train_df)}\")\n",
    "print(f\"‚úÖ Test videos loaded: {len(test_df)}\")\n",
    "\n",
    "if len(train_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LABEL DISTRIBUTION ANALYSIS (Training Data)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal training samples: {len(train_df)}\")\n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    print(train_df['label_name'].value_counts())\n",
    "    print(f\"\\nLabel Percentages:\")\n",
    "    print(train_df['label_name'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Check if we have both classes\n",
    "    unique_labels = train_df['label'].unique()\n",
    "    print(f\"\\nUnique labels found: {unique_labels}\")\n",
    "    \n",
    "    if len(unique_labels) == 1:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Only one class found in the dataset!\")\n",
    "        print(f\"   All samples are labeled as: {train_df['label_name'].iloc[0]}\")\n",
    "    else:\n",
    "        print(\"\\n‚úì Both classes found in the dataset!\")\n",
    "        print(\"   The dataset contains both Celeb-Real and Fake videos.\")\n",
    "    \n",
    "    # Show sample paths\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE PATHS (First 10):\")\n",
    "    print(\"=\"*80)\n",
    "    for idx, row in train_df.head(10).iterrows():\n",
    "        print(f\"Sample {idx}: {row['label_name']}\")\n",
    "        print(f\"  Path: {row['video_path']}\")\n",
    "    \n",
    "    # Visualize label distribution\n",
    "    if len(train_df) > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Count plot\n",
    "        label_counts = train_df['label_name'].value_counts()\n",
    "        axes[0].bar(label_counts.index, label_counts.values, color=['#2ecc71', '#e74c3c'])\n",
    "        axes[0].set_title('Label Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Label', fontsize=12)\n",
    "        axes[0].set_ylabel('Count', fontsize=12)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Pie chart\n",
    "        colors = ['#2ecc71', '#e74c3c']\n",
    "        axes[1].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%',\n",
    "                   colors=colors[:len(label_counts)], startangle=90)\n",
    "        axes[1].set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('label_distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úì Training dataset loaded: {len(train_df)} videos\")\n",
    "    print(f\"‚úì Test dataset loaded: {len(test_df)} videos\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No training videos found!\")\n",
    "    print(\"Please ensure the dataset folders are downloaded and placed in the 'data/' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Check dataset structure and video files\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET STRUCTURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'train_df' in locals() and len(train_df) > 0:\n",
    "    print(f\"\\n‚úÖ Training Dataset Structure:\")\n",
    "    print(f\"   Total videos: {len(train_df)}\")\n",
    "    print(f\"   Columns: {train_df.columns.tolist()}\")\n",
    "    print(f\"\\n   Label distribution:\")\n",
    "    print(train_df['label_name'].value_counts())\n",
    "    \n",
    "    # Check video file formats\n",
    "    print(f\"\\nüìπ Video File Formats:\")\n",
    "    train_df['extension'] = train_df['video_path'].apply(lambda x: Path(x).suffix.lower())\n",
    "    print(train_df['extension'].value_counts())\n",
    "    \n",
    "    # Sample video paths by label\n",
    "    print(f\"\\nüìÅ Sample Video Paths by Label:\")\n",
    "    for label_name in train_df['label_name'].unique():\n",
    "        print(f\"\\n   {label_name} videos:\")\n",
    "        sample_paths = train_df[train_df['label_name'] == label_name]['video_path'].head(3)\n",
    "        for path in sample_paths:\n",
    "            print(f\"     - {Path(path).name}\")\n",
    "    \n",
    "    if 'test_df' in locals() and len(test_df) > 0:\n",
    "        print(f\"\\n‚úÖ Test Dataset Structure:\")\n",
    "        print(f\"   Total test videos: {len(test_df)}\")\n",
    "        print(f\"   Test videos are in: {TESTING_FOLDER}\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Summary:\")\n",
    "    print(f\"   Training: {len(train_df)} videos\")\n",
    "    print(f\"   Testing: {len(test_df) if 'test_df' in locals() else 0} videos\")\n",
    "    print(f\"   Total: {len(train_df) + (len(test_df) if 'test_df' in locals() else 0)} videos\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Dataset not loaded. Please run the previous cell first.\")\n",
    "    print(\"Make sure the dataset folders are downloaded and placed in the 'data/' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract frame from video file\n",
    "def extract_frame_from_video(video_path, frame_idx=0):\n",
    "    \"\"\"Extract a frame from video file path\"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video {video_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Get total frames\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Set frame position\n",
    "        if frame_idx >= total_frames:\n",
    "            frame_idx = total_frames - 1\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if ret:\n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            return frame_rgb\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting frame from {video_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test video loading\n",
    "if 'train_df' in locals() and len(train_df) > 0:\n",
    "    print(\"Testing video frame extraction...\")\n",
    "    sample_video = train_df.iloc[0]['video_path']\n",
    "    print(f\"Sample video: {sample_video}\")\n",
    "    \n",
    "    frame = extract_frame_from_video(sample_video, frame_idx=0)\n",
    "    if frame is not None:\n",
    "        print(f\"‚úÖ Frame extracted successfully!\")\n",
    "        print(f\"   Frame shape: {frame.shape}\")\n",
    "        \n",
    "        # Display sample frame\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(frame)\n",
    "        plt.title(f\"Sample Frame from: {Path(sample_video).name}\\nLabel: {train_df.iloc[0]['label_name']}\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('sample_video_frame.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not extract frame. Check video file format.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training data available. Please load dataset first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset Class for Video Loading\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for loading videos from local folders.\n",
    "    Labels are assigned based on folder location:\n",
    "    - Celeb-Real folder ‚Üí Label 0 (\"Celeb-Real\")\n",
    "    - Celeb-Fake folder ‚Üí Label 1 (\"Fake\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, transform=None, num_frames=16, frame_interval=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame with columns ['video_path', 'label', 'label_name', 'folder']\n",
    "            transform: Optional transform to be applied on frames\n",
    "            num_frames: Number of frames to extract from each video\n",
    "            frame_interval: Interval between frames (1 = consecutive frames)\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_interval = frame_interval\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.dataframe.iloc[idx]['video_path']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "        label_name = self.dataframe.iloc[idx]['label_name']\n",
    "        folder = self.dataframe.iloc[idx]['folder']\n",
    "        \n",
    "        # Extract frames from video\n",
    "        frames = self.extract_frames(video_path)\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "        \n",
    "        # Convert list of frames to tensor\n",
    "        # Stack frames: [num_frames, C, H, W]\n",
    "        frames_tensor = torch.stack(frames)\n",
    "        \n",
    "        return {\n",
    "            'frames': frames_tensor,\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'label_name': label_name,\n",
    "            'folder': folder,\n",
    "            'video_path': video_path\n",
    "        }\n",
    "    \n",
    "    def extract_frames(self, video_path):\n",
    "        \"\"\"Extract frames from video file\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Could not open video: {video_path}\")\n",
    "        \n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Calculate frame indices to extract\n",
    "        if total_frames < self.num_frames * self.frame_interval:\n",
    "            # If video is shorter, extract all frames and pad\n",
    "            frame_indices = list(range(0, total_frames, self.frame_interval))\n",
    "        else:\n",
    "            # Extract evenly spaced frames\n",
    "            step = max(1, total_frames // (self.num_frames * self.frame_interval))\n",
    "            frame_indices = [i * step for i in range(self.num_frames)]\n",
    "        \n",
    "        for frame_idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # Convert BGR to RGB\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # Convert to PIL Image for transforms\n",
    "                frame_pil = Image.fromarray(frame_rgb)\n",
    "                frames.append(frame_pil)\n",
    "            else:\n",
    "                # If frame read fails, use last successful frame\n",
    "                if frames:\n",
    "                    frames.append(frames[-1])\n",
    "                else:\n",
    "                    # Create black frame as fallback\n",
    "                    frame_pil = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "                    frames.append(frame_pil)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Pad or trim to exact number of frames\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1] if frames else Image.new('RGB', (224, 224), (0, 0, 0)))\n",
    "        \n",
    "        frames = frames[:self.num_frames]\n",
    "        \n",
    "        return frames\n",
    "\n",
    "# Define transforms for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets if training data is available\n",
    "if 'train_df' in locals() and len(train_df) > 0:\n",
    "    print(\"Creating PyTorch datasets...\")\n",
    "    \n",
    "    # Split training data into train and validation sets\n",
    "    train_split_df, val_split_df = train_test_split(\n",
    "        train_df, \n",
    "        test_size=0.2, \n",
    "        random_state=SEED, \n",
    "        stratify=train_df['label']\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = VideoDataset(train_split_df, transform=train_transform, num_frames=16)\n",
    "    val_dataset = VideoDataset(val_split_df, transform=val_transform, num_frames=16)\n",
    "    \n",
    "    # Create test dataset if available\n",
    "    if 'test_df' in locals() and len(test_df) > 0:\n",
    "        test_dataset = VideoDataset(test_df, transform=val_transform, num_frames=16)\n",
    "        print(f\"‚úÖ Test dataset created: {len(test_dataset)} videos\")\n",
    "    else:\n",
    "        test_dataset = None\n",
    "        print(\"‚ö†Ô∏è  No test dataset available\")\n",
    "    \n",
    "    print(f\"‚úÖ Training dataset: {len(train_dataset)} videos\")\n",
    "    print(f\"‚úÖ Validation dataset: {len(val_dataset)} videos\")\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"  Train: {len(train_split_df)} videos\")\n",
    "    print(f\"  Validation: {len(val_split_df)} videos\")\n",
    "    print(f\"  Test: {len(test_df) if 'test_df' in locals() else 0} videos\")\n",
    "    \n",
    "    # Test dataset loading\n",
    "    print(\"\\nTesting dataset loading...\")\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"‚úÖ Sample loaded successfully!\")\n",
    "    print(f\"   Frames shape: {sample['frames'].shape}\")\n",
    "    print(f\"   Label: {sample['label'].item()} ({sample['label_name']})\")\n",
    "    print(f\"   Folder: {sample['folder']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training data available. Please load dataset first (run Cell 2).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake_env (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
