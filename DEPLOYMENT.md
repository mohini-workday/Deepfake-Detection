# Deployment Guide for Streamlit Dashboard

## ‚úÖ Pre-Deployment Checklist

### Files Required for Deployment

- ‚úÖ `DeepFakeDetection_Dashboard.py` - Main Streamlit application
- ‚úÖ `requirements.txt` - All Python dependencies
- ‚úÖ `README.md` - Project documentation
- ‚úÖ `.gitignore` - Git ignore rules

### Dependencies Status

All required packages are in `requirements.txt`:
- ‚úÖ Streamlit (1.25.0+)
- ‚úÖ PyTorch (2.0.0+)
- ‚úÖ Torchvision (0.15.0+)
- ‚úÖ TIMM (0.9.0+)
- ‚úÖ OpenCV (4.8.0+)
- ‚úÖ Plotly (5.14.0+)
- ‚úÖ Pandas, NumPy, PIL
- ‚úÖ All other dependencies

## üöÄ Deployment Options

### Option 1: Streamlit Cloud (Recommended)

1. **Push to GitHub** (Already done ‚úÖ)
   - Repository: https://github.com/mohini-workday/Deepfake-Detection

2. **Deploy on Streamlit Cloud**:
   - Go to: https://share.streamlit.io/
   - Sign in with GitHub
   - Click "New app"
   - Select repository: `mohini-workday/Deepfake-Detection`
   - Main file path: `DeepFakeDetection_Dashboard.py`
   - Click "Deploy"

3. **Configuration**:
   - Streamlit Cloud will automatically detect `requirements.txt`
   - No additional configuration needed

### Option 2: Heroku

1. **Create `Procfile`**:
   ```
   web: streamlit run DeepFakeDetection_Dashboard.py --server.port=$PORT --server.address=0.0.0.0
   ```

2. **Create `setup.sh`** (if needed):
   ```bash
   mkdir -p ~/.streamlit/
   echo "[server]" > ~/.streamlit/config.toml
   echo "headless = true" >> ~/.streamlit/config.toml
   ```

3. **Deploy**:
   ```bash
   heroku create your-app-name
   git push heroku main
   ```

### Option 3: Docker

1. **Create `Dockerfile`**:
   ```dockerfile
   FROM python:3.10-slim
   
   WORKDIR /app
   
   COPY requirements.txt .
   RUN pip install --no-cache-dir -r requirements.txt
   
   COPY . .
   
   EXPOSE 8501
   
   HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health
   
   ENTRYPOINT ["streamlit", "run", "DeepFakeDetection_Dashboard.py", "--server.port=8501", "--server.address=0.0.0.0"]
   ```

2. **Build and run**:
   ```bash
   docker build -t deepfake-dashboard .
   docker run -p 8501:8501 deepfake-dashboard
   ```

## ‚ö†Ô∏è Important Notes for Deployment

### Model Loading

The dashboard currently loads models with pretrained weights. For deployment:

1. **Option A**: Train models and save checkpoints, then load them
2. **Option B**: Use pretrained weights (current implementation)
3. **Option C**: Load models from a cloud storage (S3, GCS, etc.)

### Video Processing

- Videos are processed in memory (temporary files)
- Large videos may require more memory
- Consider adding file size limits in the uploader

### Performance Considerations

- Model loading is cached with `@st.cache_resource`
- First load may be slow (downloading pretrained weights)
- Consider pre-loading models or using smaller models for faster startup

## üìã Deployment Checklist

- [x] All code pushed to GitHub
- [x] `requirements.txt` includes all dependencies
- [x] Dashboard file is complete and functional
- [ ] Models trained and saved (optional, for better accuracy)
- [ ] Test deployment locally first
- [ ] Configure environment variables if needed
- [ ] Set up monitoring/logging (optional)

## üîß Local Testing Before Deployment

```bash
# Activate virtual environment
source deepfake_env/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run dashboard
streamlit run DeepFakeDetection_Dashboard.py
```

## üìù Current Status

‚úÖ **Code Status**: All code is pushed to GitHub
‚úÖ **Dependencies**: All in requirements.txt
‚úÖ **File Structure**: Complete
‚ö†Ô∏è **Models**: Using pretrained weights (models will download on first use)
‚úÖ **Ready for Deployment**: Yes, with notes above

## üéØ Recommended Deployment Steps

1. **Test locally** - Ensure everything works
2. **Deploy to Streamlit Cloud** - Easiest option
3. **Monitor performance** - Check memory usage
4. **Optimize if needed** - Add model caching, file size limits

## üìû Support

If you encounter issues during deployment:
1. Check Streamlit Cloud logs
2. Verify all dependencies in requirements.txt
3. Ensure Python version compatibility (3.8+)
4. Check model loading paths

